{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665644b6",
   "metadata": {},
   "source": [
    "## Second try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sentencepiece as spm\n",
    "#import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1bdd2",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ac728",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "with open('train.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line.strip())\n",
    "        texts.append(data['text'])\n",
    "\n",
    "with open('corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in texts:\n",
    "        f.write(text + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8371e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='corpus.txt',\n",
    "    model_prefix='tokenizer',\n",
    "    vocab_size=20000,\n",
    "    model_type='unigram',\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821907da",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d68535",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('tokenizer.model')  # –§–∞–π–ª –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca699e0",
   "metadata": {},
   "source": [
    "##### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('tokenizer.model')  # –§–∞–π–ª –º–æ–¥–µ–ª–∏\n",
    "\n",
    "# test\n",
    "text = \"–û—Ç–ª–∏—á–Ω—ã–π —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è TextCNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏!\"\n",
    "tokens = sp.Encode(text, out_type=int)  # IDs\n",
    "pieces = sp.Encode(text, out_type=str)  # –ü–æ–¥—Å–ª–æ–≤–∞\n",
    "\n",
    "print(\"–û–†–ò–ì–ò–ù–ê–õ:\", text)\n",
    "print(\"IDS:\", tokens)\n",
    "print(\"–¢–û–ö–ï–ù–´:\", pieces)\n",
    "print(\"Vocab size:\", sp.GetPieceSize())\n",
    "print(\"–î–ª–∏–Ω–∞:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde48028",
   "metadata": {},
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonLDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer_path='tokenizer.model', max_length=256):\n",
    "        self.file_path = file_path\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(tokenizer_path)\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                sample = json.loads(line.strip())\n",
    "                self.data.append({\n",
    "                    'text': sample['text'],\n",
    "                    'label': sample['label']\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        text = item['text']\n",
    "        tokens = self.sp.encode(text, out_type=int)\n",
    "\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            pad_id = self.sp.PieceToId('<pad>')\n",
    "            tokens = tokens + [pad_id] * (self.max_length - len(tokens))\n",
    "\n",
    "        text_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(item['label'], dtype=torch.long)\n",
    "\n",
    "        return text_tensor, label_tensor\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = JsonLDataset('train.jsonl')\n",
    "test_dataset = JsonLDataset('test.jsonl')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be3c5d0",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec16f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        # padding_idx=pad_idx –≥–æ–≤–æ—Ä–∏—Ç –º–æ–¥–µ–ª–∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω—É–ª–∏ (—É—Å–∫–æ—Ä–µ–Ω–∏–µ!)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Å–≤–µ—Ä—Ç–∫–∏ (–¥–ª—è 2, 3 –∏ 4 —Å–ª–æ–≤)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim, \n",
    "                      out_channels=n_filters, \n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Linear(n_filters * len(filter_sizes), output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text: [batch, len] -> embedded: [batch, len, dim] -> [batch, dim, len]\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        \n",
    "        # –°–≤–µ—Ä—Ç–∫–∞ + ReLU + MaxPool –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∏–ª—å—Ç—Ä–∞\n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c5f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = sp.GetPieceSize()\n",
    "model = TextCNN(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=300,\n",
    "    n_filters=100,\n",
    "    filter_sizes= [3,4,5],\n",
    "    output_dim=3,\n",
    "    dropout= 0.5,\n",
    "    pad_idx=0\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5620e607",
   "metadata": {},
   "source": [
    "##### Stats of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8749b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(f\"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5839d4f",
   "metadata": {},
   "source": [
    "##### Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ed189",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8699bdc",
   "metadata": {},
   "source": [
    "## THE CYCLE üåÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_path = 'textcnn_best.pth'\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (tokens, labels) in enumerate(train_loader):\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "        \n",
    "        # 1. –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward\n",
    "        predictions = model(tokens)\n",
    "        \n",
    "        # 3. Loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # 4. Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞\n",
    "        optimizer.step()\n",
    "        \n",
    "        # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = predictions.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    if 1==1:\n",
    "        model.eval()\n",
    "\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (tokens, labels) in enumerate(test_loader):\n",
    "                tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "                predictions = model(tokens)\n",
    "                loss = criterion(predictions, labels)\n",
    "\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = predictions.max(1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        test_acc = 100. * test_correct / test_total\n",
    "        val_loss_avg = test_loss / len(test_loader)\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f'‚úÖ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø! Val Loss: {val_loss_avg:.4f}')\n",
    "        print(f'EpochT: {epoch+1:02}, Loss: {test_loss/len(test_loader):.4f}, Acc: {test_acc:.2f}%')\n",
    "            \n",
    "    \n",
    "    # –≠–ø–æ—Ö–∞ –∑–∞–∫–æ–Ω—á–µ–Ω–∞\n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    print(f'Epoch: {epoch+1:02}, Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%')\n",
    "\n",
    "print(\"‚úÖDONE‚úÖ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
