{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665644b6",
   "metadata": {},
   "source": [
    "## Second try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dffd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sentencepiece as spm\n",
    "#import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1bdd2",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ac728",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "with open('train.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line.strip())\n",
    "        texts.append(data['text'])\n",
    "\n",
    "with open('corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in texts:\n",
    "        f.write(text + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8371e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 49021 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=6022681\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9506% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=129\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999506\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 49019 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=3035352\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 131506 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 49019\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 103550\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 103550 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=44326 obj=12.3271 num_tokens=240956 num_tokens/piece=5.436\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=39590 obj=9.58989 num_tokens=243410 num_tokens/piece=6.14827\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=29686 obj=9.61341 num_tokens=254206 num_tokens/piece=8.56316\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=29660 obj=9.58885 num_tokens=254228 num_tokens/piece=8.57141\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22243 obj=9.71559 num_tokens=270533 num_tokens/piece=12.1626\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22242 obj=9.68962 num_tokens=270525 num_tokens/piece=12.1628\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22000 obj=9.69201 num_tokens=271054 num_tokens/piece=12.3206\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=21999 obj=9.6923 num_tokens=271055 num_tokens/piece=12.3212\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: tokenizer.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='corpus.txt',\n",
    "    model_prefix='tokenizer',\n",
    "    vocab_size=20000,\n",
    "    model_type='unigram',\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca699e0",
   "metadata": {},
   "source": [
    "##### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d7b252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ОРИГИНАЛ: Отличный русский текст для TextCNN классификации!\n",
      "IDS: [1069, 403, 12968, 78, 3124, 13140, 1, 1213, 36, 2111, 1554, 9090, 10]\n",
      "ТОКЕНЫ: ['▁Отличный', '▁русский', '▁текст', '▁для', '▁T', 'ext', 'CNN', '▁класс', 'и', 'ф', 'ик', 'ации', '!']\n",
      "Vocab size: 20000\n",
      "Длина: 13\n"
     ]
    }
   ],
   "source": [
    "# Загрузка\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('tokenizer.model')  # Файл модели\n",
    "\n",
    "# test\n",
    "text = \"Отличный русский текст для TextCNN классификации!\"\n",
    "tokens = sp.Encode(text, out_type=int)  # IDs\n",
    "pieces = sp.Encode(text, out_type=str)  # Подслова\n",
    "\n",
    "print(\"ОРИГИНАЛ:\", text)\n",
    "print(\"IDS:\", tokens)\n",
    "print(\"ТОКЕНЫ:\", pieces)\n",
    "print(\"Vocab size:\", sp.GetPieceSize())\n",
    "print(\"Длина:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde48028",
   "metadata": {},
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonLDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer_path='tokenizer.model', max_length=256):\n",
    "        self.file_path = file_path\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(tokenizer_path)\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                sample = json.loads(line.strip())\n",
    "                self.data.append({\n",
    "                    'text': sample['text'],\n",
    "                    'label': sample['label']\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        text = item['text']\n",
    "        tokens = self.sp.encode(text, out_type=int)\n",
    "\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            pad_id = self.sp.PieceToId('<pad>')\n",
    "            tokens = tokens + [pad_id] * (self.max_length - len(tokens))\n",
    "\n",
    "        text_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(item['label'], dtype=torch.long)\n",
    "\n",
    "        return text_tensor, label_tensor\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = JsonLDataset('train.jsonl')\n",
    "test_dataset = JsonLDataset('test.jsonl')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be3c5d0",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec16f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
