{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665644b6",
   "metadata": {},
   "source": [
    "## Second try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dffd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sentencepiece as spm\n",
    "#import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1bdd2",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ac728",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "with open('train.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line.strip())\n",
    "        texts.append(data['text'])\n",
    "\n",
    "with open('corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in texts:\n",
    "        f.write(text + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8371e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='corpus.txt',\n",
    "    model_prefix='tokenizer',\n",
    "    vocab_size=20000,\n",
    "    model_type='unigram',\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821907da",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d68535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('tokenizer.model')  # –§–∞–π–ª –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca699e0",
   "metadata": {},
   "source": [
    "##### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('tokenizer.model')  # –§–∞–π–ª –º–æ–¥–µ–ª–∏\n",
    "\n",
    "# test\n",
    "text = \"–û—Ç–ª–∏—á–Ω—ã–π —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è TextCNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏!\"\n",
    "tokens = sp.Encode(text, out_type=int)  # IDs\n",
    "pieces = sp.Encode(text, out_type=str)  # –ü–æ–¥—Å–ª–æ–≤–∞\n",
    "\n",
    "print(\"–û–†–ò–ì–ò–ù–ê–õ:\", text)\n",
    "print(\"IDS:\", tokens)\n",
    "print(\"–¢–û–ö–ï–ù–´:\", pieces)\n",
    "print(\"Vocab size:\", sp.GetPieceSize())\n",
    "print(\"–î–ª–∏–Ω–∞:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde48028",
   "metadata": {},
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b331144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonLDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer_path='tokenizer.model', max_length=256):\n",
    "        self.file_path = file_path\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(tokenizer_path)\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                sample = json.loads(line.strip())\n",
    "                self.data.append({\n",
    "                    'text': sample['text'],\n",
    "                    'label': sample['label']\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        text = item['text']\n",
    "        tokens = self.sp.encode(text, out_type=int)\n",
    "\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            pad_id = self.sp.PieceToId('<pad>')\n",
    "            tokens = tokens + [pad_id] * (self.max_length - len(tokens))\n",
    "\n",
    "        text_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(item['label'], dtype=torch.long)\n",
    "\n",
    "        return text_tensor, label_tensor\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = JsonLDataset('train.jsonl')\n",
    "test_dataset = JsonLDataset('test.jsonl')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be3c5d0",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec16f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        # padding_idx=pad_idx –≥–æ–≤–æ—Ä–∏—Ç –º–æ–¥–µ–ª–∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω—É–ª–∏ (—É—Å–∫–æ—Ä–µ–Ω–∏–µ!)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Å–≤–µ—Ä—Ç–∫–∏ (–¥–ª—è 2, 3 –∏ 4 —Å–ª–æ–≤)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim, \n",
    "                      out_channels=n_filters, \n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Linear(n_filters * len(filter_sizes), output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text: [batch, len] -> embedded: [batch, len, dim] -> [batch, dim, len]\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        \n",
    "        # –°–≤–µ—Ä—Ç–∫–∞ + ReLU + MaxPool –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∏–ª—å—Ç—Ä–∞\n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b59c5f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = sp.GetPieceSize()\n",
    "model = TextCNN(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=300,\n",
    "    n_filters=100,\n",
    "    filter_sizes= [3,4,5],\n",
    "    output_dim=3,\n",
    "    dropout= 0.5,\n",
    "    pad_idx=0\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5620e607",
   "metadata": {},
   "source": [
    "##### Stats of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8749b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN(\n",
      "  (embedding): Embedding(20000, 300, padding_idx=0)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
      "    (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
      "    (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: 6361203\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(f\"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5839d4f",
   "metadata": {},
   "source": [
    "##### Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "441ed189",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8699bdc",
   "metadata": {},
   "source": [
    "## THE CYCLE üåÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a51c25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|‚ñà         | 1/10 [03:59<35:55, 239.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø! Val Loss: 0.6556\n",
      "EpochT: 01, Loss: 0.6556, Acc: 69.34%\n",
      "Epoch: 01, Loss: 0.8088, Acc: 62.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|‚ñà‚ñà        | 2/10 [08:20<33:38, 252.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø! Val Loss: 0.6227\n",
      "EpochT: 02, Loss: 0.6227, Acc: 72.07%\n",
      "Epoch: 02, Loss: 0.6512, Acc: 70.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|‚ñà‚ñà‚ñà       | 3/10 [12:52<30:29, 261.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpochT: 03, Loss: 0.6240, Acc: 72.00%\n",
      "Epoch: 03, Loss: 0.5683, Acc: 75.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [17:47<27:25, 274.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpochT: 04, Loss: 0.6714, Acc: 71.93%\n",
      "Epoch: 04, Loss: 0.4945, Acc: 78.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [22:14<22:39, 272.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpochT: 05, Loss: 0.7202, Acc: 71.85%\n",
      "Epoch: 05, Loss: 0.4267, Acc: 82.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [26:44<18:05, 271.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpochT: 06, Loss: 0.8431, Acc: 70.37%\n",
      "Epoch: 06, Loss: 0.3628, Acc: 85.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [31:36<13:53, 277.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpochT: 07, Loss: 0.9575, Acc: 70.27%\n",
      "Epoch: 07, Loss: 0.3083, Acc: 87.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [36:15<09:16, 278.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpochT: 08, Loss: 1.0377, Acc: 70.23%\n",
      "Epoch: 08, Loss: 0.2726, Acc: 89.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [40:45<04:35, 275.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpochT: 09, Loss: 1.2088, Acc: 69.53%\n",
      "Epoch: 09, Loss: 0.2380, Acc: 90.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [45:03<00:00, 270.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpochT: 10, Loss: 1.2529, Acc: 69.66%\n",
      "Epoch: 10, Loss: 0.2182, Acc: 91.62%\n",
      "‚úÖDONE‚úÖ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_path = 'textcnn_best.pth'\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (tokens, labels) in enumerate(train_loader):\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "        \n",
    "        # 1. –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward\n",
    "        predictions = model(tokens)\n",
    "        \n",
    "        # 3. Loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # 4. Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞\n",
    "        optimizer.step()\n",
    "        \n",
    "        # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = predictions.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    if 1==1:\n",
    "        model.eval()\n",
    "\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (tokens, labels) in enumerate(test_loader):\n",
    "                tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "                predictions = model(tokens)\n",
    "                loss = criterion(predictions, labels)\n",
    "\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = predictions.max(1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        test_acc = 100. * test_correct / test_total\n",
    "        val_loss_avg = test_loss / len(test_loader)\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f'‚úÖ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø! Val Loss: {val_loss_avg:.4f}')\n",
    "        print(f'EpochT: {epoch+1:02}, Loss: {test_loss/len(test_loader):.4f}, Acc: {test_acc:.2f}%')\n",
    "            \n",
    "    \n",
    "    # –≠–ø–æ—Ö–∞ –∑–∞–∫–æ–Ω—á–µ–Ω–∞\n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    print(f'Epoch: {epoch+1:02}, Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%')\n",
    "\n",
    "print(\"‚úÖDONE‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5950e1",
   "metadata": {},
   "source": [
    "## Extra data üóÉÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f3e62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_dataset = JsonLDataset('train2.jsonl')\n",
    "test_dataset = JsonLDataset('test.jsonl')\n",
    "\n",
    "train2_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633565d",
   "metadata": {},
   "source": [
    "### Cycle ‚Ññ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e1b5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('textcnn_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72e2d442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|‚ñà‚ñà        | 1/5 [04:38<18:32, 278.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø! Val Loss: 0.6520\n",
      "EpochT: 01, Loss: 0.6520, Acc: 71.68%\n",
      "Epoch: 01, Loss: 0.5805, Acc: 74.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [08:58<13:23, 267.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpochT: 02, Loss: 0.6641, Acc: 72.16%\n",
      "Epoch: 02, Loss: 0.5073, Acc: 78.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [13:28<08:57, 268.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpochT: 03, Loss: 0.7340, Acc: 71.17%\n",
      "Epoch: 03, Loss: 0.4387, Acc: 81.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [17:55<04:28, 268.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpochT: 04, Loss: 0.7917, Acc: 71.16%\n",
      "Epoch: 04, Loss: 0.3780, Acc: 84.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [22:28<00:00, 269.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpochT: 05, Loss: 0.8848, Acc: 70.37%\n",
      "Epoch: 05, Loss: 0.3227, Acc: 87.22%\n",
      "‚úÖDONE‚úÖ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_path = 'textcnn_best.pth'\n",
    "\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (tokens, labels) in enumerate(train2_loader):\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "        \n",
    "        # 1. –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward\n",
    "        predictions = model(tokens)\n",
    "        \n",
    "        # 3. Loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # 4. Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞\n",
    "        optimizer.step()\n",
    "        \n",
    "        # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = predictions.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    if 1==1:\n",
    "        model.eval()\n",
    "\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (tokens, labels) in enumerate(test_loader):\n",
    "                tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "                predictions = model(tokens)\n",
    "                loss = criterion(predictions, labels)\n",
    "\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = predictions.max(1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        test_acc = 100. * test_correct / test_total\n",
    "        val_loss_avg = test_loss / len(test_loader)\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f'‚úÖ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø! Val Loss: {val_loss_avg:.4f}')\n",
    "        print(f'EpochT: {epoch+1:02}, Loss: {test_loss/len(test_loader):.4f}, Acc: {test_acc:.2f}%')\n",
    "            \n",
    "    \n",
    "    # –≠–ø–æ—Ö–∞ –∑–∞–∫–æ–Ω—á–µ–Ω–∞\n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    print(f'Epoch: {epoch+1:02}, Loss: {train_loss/len(train2_loader):.4f}, Acc: {train_acc:.2f}%')\n",
    "\n",
    "print(\"‚úÖDONE‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c6d1b",
   "metadata": {},
   "source": [
    "## MORE DATA üóÉÔ∏èüóÉÔ∏èüóÉÔ∏èüóÉÔ∏èüóÉÔ∏èüóÉÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a965434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
