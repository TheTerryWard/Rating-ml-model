{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665644b6",
   "metadata": {},
   "source": [
    "## Second try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6dffd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1bdd2",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ac728",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "with open('train.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line.strip())\n",
    "        texts.append(data['text'])\n",
    "\n",
    "with open('corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in texts:\n",
    "        f.write(text + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8371e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='corpus.txt',\n",
    "    model_prefix='tokenizer',\n",
    "    vocab_size=20000,\n",
    "    model_type='unigram',\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821907da",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97d68535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('tokenizer.model')  # –§–∞–π–ª –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca699e0",
   "metadata": {},
   "source": [
    "##### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('tokenizer.model')  # –§–∞–π–ª –º–æ–¥–µ–ª–∏\n",
    "\n",
    "# test\n",
    "text = \"–û—Ç–ª–∏—á–Ω—ã–π —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è TextCNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏!\"\n",
    "tokens = sp.Encode(text, out_type=int)  # IDs\n",
    "pieces = sp.Encode(text, out_type=str)  # –ü–æ–¥—Å–ª–æ–≤–∞\n",
    "\n",
    "print(\"–û–†–ò–ì–ò–ù–ê–õ:\", text)\n",
    "print(\"IDS:\", tokens)\n",
    "print(\"–¢–û–ö–ï–ù–´:\", pieces)\n",
    "print(\"Vocab size:\", sp.GetPieceSize())\n",
    "print(\"–î–ª–∏–Ω–∞:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde48028",
   "metadata": {},
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b331144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonLDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer_path='tokenizer.model', max_length=256):\n",
    "        self.file_path = file_path\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(tokenizer_path)\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                sample = json.loads(line.strip())\n",
    "                self.data.append({\n",
    "                    'text': sample['text'],\n",
    "                    'label': sample['label']\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        text = item['text']\n",
    "        tokens = self.sp.encode(text, out_type=int)\n",
    "\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            pad_id = self.sp.PieceToId('<pad>')\n",
    "            tokens = tokens + [pad_id] * (self.max_length - len(tokens))\n",
    "\n",
    "        text_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(item['label'], dtype=torch.long)\n",
    "\n",
    "        return text_tensor, label_tensor\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = JsonLDataset('train.jsonl')\n",
    "test_dataset = JsonLDataset('test.jsonl')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be3c5d0",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bec16f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        # padding_idx=pad_idx –≥–æ–≤–æ—Ä–∏—Ç –º–æ–¥–µ–ª–∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω—É–ª–∏ (—É—Å–∫–æ—Ä–µ–Ω–∏–µ!)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Å–≤–µ—Ä—Ç–∫–∏ (–¥–ª—è 2, 3 –∏ 4 —Å–ª–æ–≤)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim, \n",
    "                      out_channels=n_filters, \n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Linear(n_filters * len(filter_sizes), output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text: [batch, len] -> embedded: [batch, len, dim] -> [batch, dim, len]\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        \n",
    "        # –°–≤–µ—Ä—Ç–∫–∞ + ReLU + MaxPool –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∏–ª—å—Ç—Ä–∞\n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b59c5f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = sp.GetPieceSize()\n",
    "model = TextCNN(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=300,\n",
    "    n_filters=100,\n",
    "    filter_sizes= [3,4,5],\n",
    "    output_dim=3,\n",
    "    dropout= 0.5,\n",
    "    pad_idx=0\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5620e607",
   "metadata": {},
   "source": [
    "##### Stats of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8749b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(f\"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5839d4f",
   "metadata": {},
   "source": [
    "##### Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "441ed189",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8699bdc",
   "metadata": {},
   "source": [
    "## THE CYCLE üåÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_path = 'textcnn_best.pth'\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (tokens, labels) in enumerate(train_loader):\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "        \n",
    "        # 1. –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward\n",
    "        predictions = model(tokens)\n",
    "        \n",
    "        # 3. Loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # 4. Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞\n",
    "        optimizer.step()\n",
    "        \n",
    "        # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = predictions.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    if 1==1:\n",
    "        model.eval()\n",
    "\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (tokens, labels) in enumerate(test_loader):\n",
    "                tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "                predictions = model(tokens)\n",
    "                loss = criterion(predictions, labels)\n",
    "\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = predictions.max(1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        test_acc = 100. * test_correct / test_total\n",
    "        val_loss_avg = test_loss / len(test_loader)\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f'‚úÖ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø! Val Loss: {val_loss_avg:.4f}')\n",
    "        print(f'EpochT: {epoch+1:02}, Loss: {test_loss/len(test_loader):.4f}, Acc: {test_acc:.2f}%')\n",
    "            \n",
    "    \n",
    "    # –≠–ø–æ—Ö–∞ –∑–∞–∫–æ–Ω—á–µ–Ω–∞\n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    print(f'Epoch: {epoch+1:02}, Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%')\n",
    "\n",
    "print(\"‚úÖDONE‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5950e1",
   "metadata": {},
   "source": [
    "## Extra data üóÉÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_dataset = JsonLDataset('train2.jsonl')\n",
    "test_dataset = JsonLDataset('test.jsonl')\n",
    "\n",
    "train2_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633565d",
   "metadata": {},
   "source": [
    "### Cycle ‚Ññ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('textcnn_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e2d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_path = 'textcnn_best.pth'\n",
    "\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (tokens, labels) in enumerate(train2_loader):\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "        \n",
    "        # 1. –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward\n",
    "        predictions = model(tokens)\n",
    "        \n",
    "        # 3. Loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # 4. Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞\n",
    "        optimizer.step()\n",
    "        \n",
    "        # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = predictions.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    if 1==1:\n",
    "        model.eval()\n",
    "\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (tokens, labels) in enumerate(test_loader):\n",
    "                tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "                predictions = model(tokens)\n",
    "                loss = criterion(predictions, labels)\n",
    "\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = predictions.max(1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        test_acc = 100. * test_correct / test_total\n",
    "        val_loss_avg = test_loss / len(test_loader)\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f'‚úÖ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø! Val Loss: {val_loss_avg:.4f}')\n",
    "        print(f'EpochT: {epoch+1:02}, Loss: {test_loss/len(test_loader):.4f}, Acc: {test_acc:.2f}%')\n",
    "            \n",
    "    \n",
    "    # –≠–ø–æ—Ö–∞ –∑–∞–∫–æ–Ω—á–µ–Ω–∞\n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    print(f'Epoch: {epoch+1:02}, Loss: {train_loss/len(train2_loader):.4f}, Acc: {train_acc:.2f}%')\n",
    "\n",
    "print(\"‚úÖDONE‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c6d1b",
   "metadata": {},
   "source": [
    "## MORE DATA üóÉÔ∏èüóÉÔ∏èüóÉÔ∏èüóÉÔ∏èüóÉÔ∏èüóÉÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a965434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('textcnn_best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fc7d60",
   "metadata": {},
   "source": [
    "#### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a69b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('YandexMAPSdatasetCL.parquet')\n",
    "\n",
    "print(\"Size:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "half_df = df.sample(frac=0.5, random_state=42) # half\n",
    "\n",
    "def rating_to_numeric(rating):\n",
    "    if rating <= 2:\n",
    "        return 0  # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π\n",
    "    elif rating == 3:\n",
    "        return 1  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π\n",
    "    else:  # 4 –∏–ª–∏ 5\n",
    "        return 2  # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π\n",
    "\n",
    "half_df['label'] = half_df['rating'].apply(rating_to_numeric)  # new 'label'\n",
    "half_df.to_parquet('halfYANDEXDATASET.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38fb371",
   "metadata": {},
   "outputs": [],
   "source": [
    "half_df = pd.read_parquet('halfYANDEXDATASET.parquet')\n",
    "\n",
    "indices = np.arange(len(half_df))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split_idx = int(0.7 * len(half_df))\n",
    "train_indices = indices[:split_idx]\n",
    "test_indices = indices[split_idx:]\n",
    "\n",
    "train_df = half_df.iloc[train_indices][['text', 'label']].reset_index(drop=True)\n",
    "test_df = half_df.iloc[test_indices][['text', 'label']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_df.to_parquet('train.parquet', index=False)\n",
    "test_df.to_parquet('test.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ff5a5",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9551ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetLDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer_path='tokenizer.model', max_length=256):\n",
    "        self.file_path = file_path\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.df = pd.read_parquet(file_path)        \n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(tokenizer_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['text']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "\n",
    "        tokens = self.sp.encode(text, out_type=int)\n",
    "\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            pad_id = self.sp.PieceToId('<pad>')\n",
    "            tokens = tokens + [pad_id] * (self.max_length - len(tokens))\n",
    "\n",
    "        return torch.tensor(tokens, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "948b3585",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ParquetLDataset('train.parquet')\n",
    "test_dataset = ParquetLDataset('test.parquet')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3d5ac",
   "metadata": {},
   "source": [
    "### NEW OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3e9705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',          # —Å–ª–µ–¥–∏–º –∑–∞ val_loss (–º–∏–Ω–∏–º—É–º)\n",
    "    factor=0.5,          # LR √ó 0.5 –ø—Ä–∏ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏\n",
    "    patience=3,          # 3 —ç–ø–æ—Ö–∏ –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è ‚Üí –ø–æ–Ω–∏–∂–∞–µ–º\n",
    "    min_lr=1e-6          # –º–∏–Ω–∏–º—É–º LR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3445bd1",
   "metadata": {},
   "source": [
    "## Cycle 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_path = 'textcnn_best.pth'\n",
    "\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (tokens, labels) in enumerate(train_loader):\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "        \n",
    "        # 1. –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward\n",
    "        predictions = model(tokens)\n",
    "        \n",
    "        # 3. Loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # 4. Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞\n",
    "        optimizer.step()\n",
    "        \n",
    "        # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = predictions.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    if 1==1:\n",
    "        model.eval()\n",
    "\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (tokens, labels) in enumerate(test_loader):\n",
    "                tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "                predictions = model(tokens)\n",
    "                loss = criterion(predictions, labels)\n",
    "\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = predictions.max(1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        test_acc = 100. * test_correct / test_total\n",
    "        val_loss_avg = test_loss / len(test_loader)\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f'‚úÖ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø! Val Loss: {val_loss_avg:.4f}')\n",
    "        print(f'EpochT: {epoch+1:02}, Loss: {test_loss/len(test_loader):.4f}, Acc: {test_acc:.2f}%')\n",
    "        scheduler.step(val_loss_avg)\n",
    "            \n",
    "    \n",
    "    # –≠–ø–æ—Ö–∞ –∑–∞–∫–æ–Ω—á–µ–Ω–∞\n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    print(f'Epoch: {epoch+1:02}, Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%')\n",
    "\n",
    "print(\"‚úÖDONE‚úÖ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
